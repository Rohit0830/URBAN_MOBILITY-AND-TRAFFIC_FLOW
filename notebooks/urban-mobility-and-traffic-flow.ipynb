{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1766dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60e90fc7",
   "metadata": {},
   "source": [
    "## Read & inspect datasets\n",
    "\n",
    "This section adds quick checks to load a small sample from each CSV, print dtypes, and produce a chunked missing-value summary for large files. Run the cells below to get a concise report on schema, sizes, and missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba79259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: quick sample + dtype check\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_dir = Path('..') / 'dataset'\n",
    "files = {\n",
    "    'sampled_probe': dataset_dir / 'sampled_probe.csv',\n",
    "    'synthetic_probe': dataset_dir / 'synthetic_probe_data.csv',\n",
    "    'traffic_counts': dataset_dir / 'synthetic_traffic_counts.csv',\n",
    "    'accidents': dataset_dir / 'US_Accidents_March23.csv'\n",
    "}\n",
    "\n",
    "def sample_info(path, n=5):\n",
    "    print(f\"--- {path.name} ---\")\n",
    "    try:\n",
    "        df = pd.read_csv(path, nrows=n)\n",
    "        print('shape:', (len(df), len(df.columns)))\n",
    "        print('dtypes:')\n",
    "        print(df.dtypes)\n",
    "        print('\\nfirst row sample:')\n",
    "        print(df.head(1).to_dict(orient='records')[0])\n",
    "    except Exception as e:\n",
    "        print('Error sampling:', e)\n",
    "\n",
    "for name, p in files.items():\n",
    "    sample_info(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d0160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: chunked summarizer (missingness + counts)\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def summarize_csv(path, usecols=None, chunksize=200_000, max_chunks=None):\n",
    "    print(f'=== Summarizing {path.name} ===')\n",
    "    try:\n",
    "        total = 0\n",
    "        nulls = Counter()\n",
    "        dtypes = None\n",
    "        chunks = 0\n",
    "        for chunk in pd.read_csv(path, usecols=usecols, chunksize=chunksize, low_memory=False):\n",
    "            chunks += 1\n",
    "            total += len(chunk)\n",
    "            if dtypes is None:\n",
    "                dtypes = chunk.dtypes\n",
    "            nulls_chunk = chunk.isna().sum()\n",
    "            for col, n in nulls_chunk.items():\n",
    "                nulls[col] += int(n)\n",
    "            if max_chunks and chunks >= max_chunks:\n",
    "                break\n",
    "        print('rows inspected:', total)\n",
    "        if dtypes is not None:\n",
    "            print('\\ndtypes (sample):')\n",
    "            print(dtypes.head(20))\n",
    "        print('\\nTop missing columns:')\n",
    "        for col, n in Counter(nulls).most_common(20):\n",
    "            print(f\"{col}: {n}\")\n",
    "    except Exception as e:\n",
    "        print('Error summarizing:', e)\n",
    "\n",
    "# Run summarizer on a few files (limited pass for accidents to avoid long runtime)\n",
    "summarize_csv(files['sampled_probe'])\n",
    "summarize_csv(files['traffic_counts'], max_chunks=2)\n",
    "summarize_csv(files['accidents'], usecols=['ID','Start_Time','End_Time','Start_Lat','Start_Lng','End_Lat','End_Lng','Temperature(F)','Precipitation(in)','Visibility(mi)','Weather_Condition','Amenity','Traffic_Signal'], max_chunks=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c3baaa",
   "metadata": {},
   "source": [
    "## Cleaning: `US_Accidents_March23.csv`\n",
    "\n",
    "This section defines a reusable cleaning function for the accidents dataset and demonstrates it on a manageable sample. The function parses datetimes, coerces numeric fields, normalizes booleans and zip codes, validates coordinates, optionally fills missing end coordinates, and documents the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe37dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning function for accidents dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "\n",
    "BOOL_COLS = ['Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal','Turning_Loop']\n",
    "NUMERIC_COLS = ['Temperature(F)','Wind_Chill(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)','Precipitation(in)','Distance(mi)']\n",
    "DATE_COLS = ['Start_Time','End_Time','Weather_Timestamp']\n",
    "\n",
    "\n",
    "def _to_bool_series(s: pd.Series) -> pd.Series:\n",
    "    # Robust mapping: treats 'true','1','yes' as True; everything else False\n",
    "    return s.fillna('').astype(str).str.strip().str.lower().isin({'true','1','t','yes'})\n",
    "\n",
    "\n",
    "def _validate_latlng(s: pd.Series, minv: float, maxv: float) -> pd.Series:\n",
    "    s = pd.to_numeric(s, errors='coerce')\n",
    "    s[(s < minv) | (s > maxv)] = np.nan\n",
    "    return s\n",
    "\n",
    "\n",
    "def clean_accidents_df(df: pd.DataFrame, fill_end_with_start: bool = False, drop_missing_start_coords: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Return a cleaned copy of the accidents DataFrame.\n",
    "\n",
    "    Steps:\n",
    "    - Parse dates in DATE_COLS (coerce errors to NaT)\n",
    "    - Convert NUMERIC_COLS to numeric (coerce errors to NaN)\n",
    "    - Normalize boolean flag columns listed in BOOL_COLS to actual bools\n",
    "    - Normalize Zipcode to first 5 digits (string)\n",
    "    - Validate lat/lng ranges and optionally fill missing end coords from start coords\n",
    "    - Optionally drop rows missing start coordinates (default True)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Raw dataframe loaded from CSV.\n",
    "    fill_end_with_start : bool\n",
    "        If True, missing End_Lat/End_Lng will be filled from Start_Lat/Start_Lng.\n",
    "    drop_missing_start_coords : bool\n",
    "        If True, drop rows without Start_Lat/Start_Lng.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Cleaned dataframe (copy).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # IDs and basic types\n",
    "    if 'ID' in df.columns:\n",
    "        df['ID'] = df['ID'].astype(str)\n",
    "\n",
    "    # Dates\n",
    "    for c in DATE_COLS:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors='coerce')\n",
    "\n",
    "    # Numbers\n",
    "    for c in NUMERIC_COLS:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "    # Booleans\n",
    "    for c in BOOL_COLS:\n",
    "        if c in df.columns:\n",
    "            df[c] = _to_bool_series(df[c])\n",
    "\n",
    "    # Zipcode normalization\n",
    "    if 'Zipcode' in df.columns:\n",
    "        df['Zipcode'] = df['Zipcode'].astype(str).str.extract(r'(\\d{5})')[0]\n",
    "\n",
    "    # Coordinate validation\n",
    "    if 'Start_Lat' in df.columns:\n",
    "        df['Start_Lat'] = _validate_latlng(df['Start_Lat'], -90.0, 90.0)\n",
    "    if 'Start_Lng' in df.columns:\n",
    "        df['Start_Lng'] = _validate_latlng(df['Start_Lng'], -180.0, 180.0)\n",
    "    if 'End_Lat' in df.columns:\n",
    "        df['End_Lat'] = _validate_latlng(df['End_Lat'], -90.0, 90.0)\n",
    "    if 'End_Lng' in df.columns:\n",
    "        df['End_Lng'] = _validate_latlng(df['End_Lng'], -180.0, 180.0)\n",
    "\n",
    "    # Flag for end coords\n",
    "    df['has_end_coords'] = df.get('End_Lat').notna() & df.get('End_Lng').notna() if 'End_Lat' in df.columns and 'End_Lng' in df.columns else False\n",
    "\n",
    "    # Optionally fill end with start\n",
    "    if fill_end_with_start and 'Start_Lat' in df.columns and 'Start_Lng' in df.columns and 'End_Lat' in df.columns and 'End_Lng' in df.columns:\n",
    "        missing_end = df['has_end_coords'] == False\n",
    "        df.loc[missing_end, 'End_Lat'] = df.loc[missing_end, 'Start_Lat']\n",
    "        df.loc[missing_end, 'End_Lng'] = df.loc[missing_end, 'Start_Lng']\n",
    "        df['has_end_coords'] = df.get('End_Lat').notna() & df.get('End_Lng').notna()\n",
    "\n",
    "    # Drop rows missing start coords if requested\n",
    "    if drop_missing_start_coords:\n",
    "        if 'Start_Lat' in df.columns and 'Start_Lng' in df.columns:\n",
    "            before = len(df)\n",
    "            df = df.dropna(subset=['Start_Lat','Start_Lng'])\n",
    "            # small log\n",
    "            print(f'Dropped {before - len(df)} rows with missing start coordinates')\n",
    "\n",
    "    # Add simple derived columns if applicable\n",
    "    if 'Start_Time' in df.columns:\n",
    "        df['start_hour'] = df['Start_Time'].dt.hour\n",
    "        df['start_date'] = df['Start_Time'].dt.date\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6366e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sample rows: 100000\n",
      "Raw missing top columns:\n",
      "End_Lng              100000\n",
      "End_Lat              100000\n",
      "Wind_Chill(F)         95678\n",
      "Precipitation(in)     92632\n",
      "Wind_Speed(mph)       23820\n",
      "Humidity(%)            1856\n",
      "Visibility(mi)         1846\n",
      "Weather_Condition      1604\n",
      "Temperature(F)         1591\n",
      "Pressure(in)           1292\n",
      "dtype: int64\n",
      "Dropped 0 rows with missing start coordinates\n",
      "\n",
      "After cleaning, top missing columns:\n",
      "End_Lat              100000\n",
      "End_Lng              100000\n",
      "Wind_Chill(F)         95678\n",
      "Precipitation(in)     92632\n",
      "Wind_Speed(mph)       23820\n",
      "Humidity(%)            1856\n",
      "Visibility(mi)         1846\n",
      "Weather_Condition      1604\n",
      "Temperature(F)         1591\n",
      "Pressure(in)           1292\n",
      "dtype: int64\n",
      "\n",
      "Boolean flags sample (counts):\n",
      "Amenity: {False: 99216, True: 784}\n",
      "Bump: {False: 99952, True: 48}\n",
      "Crossing: {False: 93511, True: 6489}\n",
      "Give_Way: {False: 99834, True: 166}\n",
      "Junction: {False: 89566, True: 10434}\n",
      "No_Exit: {False: 99946, True: 54}\n",
      "Railway: {False: 98738, True: 1262}\n",
      "Roundabout: {False: 99995, True: 5}\n",
      "Station: {False: 96832, True: 3168}\n",
      "Stop: {False: 96676, True: 3324}\n",
      "Traffic_Calming: {False: 99922, True: 78}\n",
      "Traffic_Signal: {False: 89301, True: 10699}\n",
      "Turning_Loop: {False: 100000}\n",
      "Saved cleaned sample to ..\\dataset\\accidents_sample_cleaned.parquet\n"
     ]
    }
   ],
   "source": [
    "# Demo: run cleaning on a sample and save cleaned sample\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "acc_path = Path('..') / 'dataset' / 'US_Accidents_March23.csv'\n",
    "out_path = Path('..') / 'dataset' / 'accidents_sample_cleaned'\n",
    "\n",
    "# Choose manageable sample size for quick runs\n",
    "nrows = 100_000\n",
    "print('Loading sample rows:', nrows)\n",
    "raw = pd.read_csv(acc_path, nrows=nrows)\n",
    "print('Raw missing top columns:')\n",
    "print(raw.isna().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "cleaned = clean_accidents_df(raw, fill_end_with_start=False)\n",
    "print('\\nAfter cleaning, top missing columns:')\n",
    "print(cleaned.isna().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "print('\\nBoolean flags sample (counts):')\n",
    "for c in BOOL_COLS:\n",
    "    if c in cleaned.columns:\n",
    "        print(f\"{c}:\", cleaned[c].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# Save cleaned sample (try parquet first, fall back to csv)\n",
    "out_parquet = out_path.with_suffix('.parquet')\n",
    "out_csv = out_path.with_suffix('.csv')\n",
    "\n",
    "try:\n",
    "    cleaned.to_parquet(out_parquet)\n",
    "    print('Saved cleaned sample to', out_parquet)\n",
    "except Exception as e:\n",
    "    print('Parquet write failed (missing dependency?), falling back to CSV. Error:', e)\n",
    "    cleaned.to_csv(out_csv, index=False)\n",
    "    print('Saved cleaned sample to', out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6de46f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running full pipeline: this may take several minutes depending on file size and system resources\n",
      "Starting pipeline... (this will print progress)\n",
      "Computing city-level medians for weather columns (this may take a while)...\n",
      "Computed medians from 7728394 rows: overall medians sample: {'Temperature(F)': np.float64(68.0), 'Humidity(%)': np.float64(63.0), 'Pressure(in)': np.float64(29.95)}\n",
      "Processed 500000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1500000 rows...\n",
      "Processed 2000000 rows...\n",
      "Processed 2500000 rows...\n",
      "Processed 3000000 rows...\n",
      "Processed 3500000 rows...\n",
      "Processed 4000000 rows...\n",
      "Processed 4500000 rows...\n",
      "Processed 5000000 rows...\n",
      "Processed 5500000 rows...\n",
      "Processed 6000000 rows...\n",
      "Processed 6500000 rows...\n",
      "Processed 7000000 rows...\n",
      "Processed 7500000 rows...\n",
      "Processed 7728394 rows...\n",
      "Pipeline complete. Output written to dataset\\cleand-data\\US_Accidents_March23_cleaned.csv\n",
      "Pipeline finished\n",
      "Restored working directory\n"
     ]
    }
   ],
   "source": [
    "# Run full end-to-end cleaning pipeline (robust execution)\n",
    "print('Running full pipeline: this may take several minutes depending on file size and system resources')\n",
    "import runpy\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ensure we run from project root so relative paths inside the script work\n",
    "cwd = Path.cwd()\n",
    "project_root = cwd.parent  # notebooks/.. => project root\n",
    "os.chdir(project_root)\n",
    "try:\n",
    "    print('Starting pipeline... (this will print progress)')\n",
    "    runpy.run_path('tools/accidents_pipeline.py', run_name='__main__')\n",
    "    print('Pipeline finished')\n",
    "finally:\n",
    "    os.chdir(cwd)\n",
    "    print('Restored working directory')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
